{
  "name": "Bigqueryr",
  "tagline": "R Interface with Google BigQuery",
  "body": "# bigQueryR\r\n[![Travis-CI Build Status](https://travis-ci.org/MarkEdmondson1234/bigQueryR.svg?branch=master)](https://travis-ci.org/MarkEdmondson1234/bigQueryR)\r\n\r\n## Introduction \r\n\r\nThis is a package for interating with [BigQuery](https://cloud.google.com/bigquery/) from within R.\r\n\r\nYou may want instead to use [bigrquery](https://github.com/hadley/bigrquery) which is more developed with integration with `dplyr` etc. Some functions from `bigrquery` are used in this package.\r\n\r\n### Why this package then?\r\n\r\nThis package is here as it uses [googleAuthR](https://github.com/MarkEdmondson1234/googleAuthR) as backend, so has Shiny support, and compatibility with other googleAuthR dependent packages.\r\n\r\nIt also has support for data extracts to Google Cloud Storage, meaning you can download data and make the download URL available to a user via their Google email. If you do a query normally with over 100000 results it hangs and errors. \r\n\r\nAn example of a BigQuery Shiny app running OAuth2 is here, the [BigQuery Visualiser](https://mark.shinyapps.io/bigquery-viz/)\r\n\r\n## Authentication\r\n\r\nAuthentication is as used in other `googleAuthR` libraries:\r\n\r\n```r\r\nlibrary(bigQueryR)\r\n\r\n## this will open your browser\r\n## Authenticate with an email that has access to the BigQuery project you need\r\nbqr_auth()\r\n\r\n## verify under a new user\r\nbqr_auth(new_user=TRUE)\r\n```\r\n\r\nIf you are authenticating under several APIs via `googleAuthR`then use `gar_auth()` instead with the appropriate scopes set.\r\n\r\nYou can also use service-to-service JSON files and multi-user authentication under Shiny, see the `googleAuthR` readme for details.\r\n\r\n## Listing BigQuery meta data\r\n\r\nVarious functions for listing what is in your BigQuery account.\r\n\r\n```r\r\nlibrary(bigQueryR)\r\n  \r\n## this will open your browser\r\n## Authenticate with an email that has access to the BigQuery project you need\r\nbqr_auth()\r\n  \r\n## verify under a new user\r\nbqr_auth(new_user=TRUE)\r\n  \r\n## get projects\r\nprojects <- bqr_list_projects()\r\n  \r\nmy_project <- projects[1]\r\n  \r\n## for first project, get datasets\r\ndatasets <- bqr_list_datasets[my_project]\r\n\r\nmy_dataset <- datasets[1]\r\n## list tables\r\nmy_table <- bqr_list_tables(my_project, my_dataset)\r\n\r\n## get metadata for table\r\nmeta_table <- bqr_table_meta(my_project, my_dataset, my_table)\r\n\r\n```\r\n\r\n## Simple Queries\r\n\r\nYou can pass in queries that have results under ~ 100000 rows using this command:\r\n\r\n```r\r\nbqr_query(\"big-query-r\",\"samples\",\r\n          \"SELECT COUNT(repository.url) FROM [publicdata:samples.github_nested]\")\r\n```\r\n\r\nMore than that, and the API starts to hang and you are limited by your download bandwidth.\r\n\r\n## Asynchronous Queries\r\n\r\nFor bigger queries, asynchronous queries save the results to another BigQuery table.  You can check the progress of the job via `bqr_get_job`\r\n\r\n```r\r\nlibrary(bigQueryR)\r\n\r\n## Auth with a project that has at least BigQuery and Google Cloud Storage scope\r\nbqr_auth()\r\n\r\n## make a big query\r\njob <- bqr_query_asynch(\"your_project\", \r\n                        \"your_dataset\",\r\n                        \"SELECT * FROM blah LIMIT 9999999\", \r\n                        destinationTableId = \"bigResultTable\")\r\n                        \r\n## poll the job to check its status\r\n## its done when job$status$state == \"DONE\"\r\nbqr_get_job(\"your_project\", job$jobReference$jobId)\r\n\r\n##once done, the query results are in \"bigResultTable\"\r\n```\r\n\r\nYou may now want to download this data.  For large datasets, this is best done via extracting the BigQuery result to Google Cloud Storage, then downloading the data from there. \r\n\r\nYou can create a bucket at Google Cloud Storage at <https://console.cloud.google.com/storage/browser>, or you can use [library(googleCloudStorageR)](https://github.com/MarkEdmondson1234/googleCloudStorageR)\r\n\r\nOnce created, you can extract your data via the below:\r\n\r\n```r\r\n## Create the data extract from BigQuery to Cloud Storage\r\njob_extract <- bqr_extract_data(\"your_project\",\r\n                                \"your_dataset\",\r\n                                \"bigResultTable\",\r\n                                \"your_cloud_storage_bucket_name\")\r\n                                \r\n## poll the extract job to check its status\r\n## its done when job$status$state == \"DONE\"\r\nbqr_get_job(\"your_project\", job_extract$jobReference$jobId)\r\n\r\n## to download via a URL and not logging in via Google Cloud Storage interface:\r\n## Use an email that is Google account enabled\r\n## Requires scopes:\r\n##  https://www.googleapis.com/auth/devstorage.full_control\r\n##  https://www.googleapis.com/auth/cloud-platform\r\n## set via options(\"bigQueryR.scopes\") and reauthenticate if needed\r\n\r\ndownload_url <- bqr_grant_extract_access(job_extract, \"your@email.com\")\r\n\r\n## download_url may be multiple if the data is > 1GB\r\n> [1] \"https://storage.cloud.google.com/big-query-r-extracts/extract-20160311112410-000000000000.csv\"\r\n> [2] \"https://storage.cloud.google.com/big-query-r-extracts/extract-20160311112410-000000000001.csv\"\r\n> [3] \"https://storage.cloud.google.com/big-query-r-extracts/extract-20160311112410-000000000002.csv\"\r\n\r\n```\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}