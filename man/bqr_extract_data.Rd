% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/downloadData.R
\name{bqr_extract_data}
\alias{bqr_extract_data}
\title{Extract data asynchronously}
\usage{
bqr_extract_data(projectId, datasetId, tableId, cloudStorageBucket,
  filename = paste0("big-query-extract-", gsub(" |:|-", "", Sys.time()),
  "-*.csv"), compression = c("NONE", "GZIP"), destinationFormat = c("CSV",
  "NEWLINE_DELIMITED_JSON", "AVRO"), fieldDelimiter = ",",
  printHeader = TRUE)
}
\arguments{
\item{projectId}{The BigQuery project ID.}

\item{datasetId}{A datasetId within projectId.}

\item{tableId}{ID of table you wish to extract.}

\item{cloudStorageBucket}{URI of the bucket to extract into.}

\item{filename}{Include a wildcard (*) if extract expected to be > 1GB.}

\item{compression}{Compression of file.}

\item{destinationFormat}{Format of file.}

\item{fieldDelimiter}{fieldDelimiter of file.}

\item{printHeader}{Whether to include header row.}
}
\value{
A Job object to be queried via \link{bqr_get_job}
}
\description{
Use this instead of \link{bqr_query} for big datasets. 
Requires you to make a bucket at https://console.cloud.google.com/storage/browser
}
\seealso{
https://cloud.google.com/bigquery/exporting-data-from-bigquery
}

